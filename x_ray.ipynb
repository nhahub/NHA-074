{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18613,"sourceType":"datasetVersion","datasetId":5839}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:01.944219Z","iopub.execute_input":"2025-11-27T06:21:01.944823Z","iopub.status.idle":"2025-11-27T06:21:05.863043Z","shell.execute_reply.started":"2025-11-27T06:21:01.944788Z","shell.execute_reply":"2025-11-27T06:21:05.862440Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2) Paths\n# ================================\nBASE = \"/kaggle/input/data\"\nCSV_PATH = os.path.join(BASE, \"Data_Entry_2017.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:43.972976Z","iopub.execute_input":"2025-11-27T06:21:43.973618Z","iopub.status.idle":"2025-11-27T06:21:43.977288Z","shell.execute_reply.started":"2025-11-27T06:21:43.973593Z","shell.execute_reply":"2025-11-27T06:21:43.976517Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"CSV_PATH","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:44.894154Z","iopub.execute_input":"2025-11-27T06:21:44.894992Z","iopub.status.idle":"2025-11-27T06:21:44.899964Z","shell.execute_reply.started":"2025-11-27T06:21:44.894965Z","shell.execute_reply":"2025-11-27T06:21:44.899350Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/data/Data_Entry_2017.csv'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 3) Select only images_001 to images_005\n# ================================\nselected_folders = [f\"images_{str(i).zfill(3)}\" for i in range(1, 6)]\n\n# Create set of existing images in selected folders\nexisting_images = set()\nfor folder in selected_folders:\n    subfolder = os.path.join(BASE, folder, \"images\")\n    if os.path.exists(subfolder):\n        for f in os.listdir(subfolder):\n            existing_images.add(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:45.780857Z","iopub.execute_input":"2025-11-27T06:21:45.781541Z","iopub.status.idle":"2025-11-27T06:21:46.739266Z","shell.execute_reply.started":"2025-11-27T06:21:45.781516Z","shell.execute_reply":"2025-11-27T06:21:46.738622Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv(CSV_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:47.793032Z","iopub.execute_input":"2025-11-27T06:21:47.793551Z","iopub.status.idle":"2025-11-27T06:21:48.028989Z","shell.execute_reply.started":"2025-11-27T06:21:47.793527Z","shell.execute_reply":"2025-11-27T06:21:48.028384Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df = df[df['Image Index'].isin(existing_images)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:48.936045Z","iopub.execute_input":"2025-11-27T06:21:48.936654Z","iopub.status.idle":"2025-11-27T06:21:48.985621Z","shell.execute_reply.started":"2025-11-27T06:21:48.936631Z","shell.execute_reply":"2025-11-27T06:21:48.985032Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df = df[df['Image Index'].isin(existing_images)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:21:50.012505Z","iopub.execute_input":"2025-11-27T06:21:50.013204Z","iopub.status.idle":"2025-11-27T06:21:50.039837Z","shell.execute_reply.started":"2025-11-27T06:21:50.013177Z","shell.execute_reply":"2025-11-27T06:21:50.039010Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"Total images after filtering: {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:03.436757Z","iopub.execute_input":"2025-11-27T06:22:03.437498Z","iopub.status.idle":"2025-11-27T06:22:03.441485Z","shell.execute_reply.started":"2025-11-27T06:22:03.437475Z","shell.execute_reply":"2025-11-27T06:22:03.440906Z"}},"outputs":[{"name":"stdout","text":"Total images after filtering: 44999\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 5) Multi-label targets\n# ================================\ndf['Finding Labels'] = df['Finding Labels'].apply(lambda x: x.split('|'))\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(df['Finding Labels'])\nclasses = mlb.classes_\nnum_classes = len(classes)\nprint(\"Classes:\", classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:05.286837Z","iopub.execute_input":"2025-11-27T06:22:05.287561Z","iopub.status.idle":"2025-11-27T06:22:05.506199Z","shell.execute_reply.started":"2025-11-27T06:22:05.287538Z","shell.execute_reply":"2025-11-27T06:22:05.505560Z"}},"outputs":[{"name":"stdout","text":"Classes: ['Atelectasis' 'Cardiomegaly' 'Consolidation' 'Edema' 'Effusion'\n 'Emphysema' 'Fibrosis' 'Hernia' 'Infiltration' 'Mass' 'No Finding'\n 'Nodule' 'Pleural_Thickening' 'Pneumonia' 'Pneumothorax']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 5) Train/Validation Split\n# ================================\ntrain_df, val_df, train_labels, val_labels = train_test_split(\n    df, labels, test_size=0.1, random_state=42\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:06.880717Z","iopub.execute_input":"2025-11-27T06:22:06.881421Z","iopub.status.idle":"2025-11-27T06:22:06.902545Z","shell.execute_reply.started":"2025-11-27T06:22:06.881397Z","shell.execute_reply":"2025-11-27T06:22:06.901968Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 7) Dataset Class (direct access, selected folders)\n# ================================\n\nclass ChestXrayDataset(Dataset):\n    def __init__(self, dataframe, labels, base_path, folders, transform=None):\n        self.df = dataframe\n        self.labels = labels\n        self.base_path = base_path\n        self.folders = folders\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx][\"Image Index\"]\n        found = False\n        for folder in self.folders:\n            img_path = os.path.join(self.base_path, folder, \"images\", img_name)\n            if os.path.exists(img_path):\n                found = True\n                break\n        if not found:\n            raise FileNotFoundError(f\"{img_name} not found in any folder\")\n\n        image = Image.open(img_path).convert(\"RGB\")\n        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:09.246128Z","iopub.execute_input":"2025-11-27T06:22:09.246707Z","iopub.status.idle":"2025-11-27T06:22:09.252164Z","shell.execute_reply.started":"2025-11-27T06:22:09.246686Z","shell.execute_reply":"2025-11-27T06:22:09.251505Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n# 8) Transforms + Data Augmentation\n# ================================\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\ntrain_dataset = ChestXrayDataset(train_df, train_labels, BASE, selected_folders, transform=train_transform)\nval_dataset = ChestXrayDataset(val_df, val_labels, BASE, selected_folders, transform=val_transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:12.896782Z","iopub.execute_input":"2025-11-27T06:22:12.897354Z","iopub.status.idle":"2025-11-27T06:22:12.902761Z","shell.execute_reply.started":"2025-11-27T06:22:12.897332Z","shell.execute_reply":"2025-11-27T06:22:12.902026Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 9) Handle Imbalance\n# ================================\nclass_counts = np.sum(train_labels, axis=0)\nweights_per_class = 1. / (class_counts + 1e-6)\nsample_weights = np.dot(train_labels, weights_per_class)\nsample_weights = torch.tensor(sample_weights, dtype=torch.float32)\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:15.876665Z","iopub.execute_input":"2025-11-27T06:22:15.877059Z","iopub.status.idle":"2025-11-27T06:22:15.895404Z","shell.execute_reply.started":"2025-11-27T06:22:15.877033Z","shell.execute_reply":"2025-11-27T06:22:15.894534Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# 10) DataLoaders\n# ================================\ntrain_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:52.082116Z","iopub.execute_input":"2025-11-27T06:22:52.082428Z","iopub.status.idle":"2025-11-27T06:22:52.086641Z","shell.execute_reply.started":"2025-11-27T06:22:52.082406Z","shell.execute_reply":"2025-11-27T06:22:52.086061Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 11) VGG16 Model Setup\n# ================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg = models.vgg16(weights=\"IMAGENET1K_V1\")\n\n# Freeze early layers\nfor param in vgg.features.parameters():\n    param.requires_grad = False\n\n# Replace classifier\nvgg.classifier[6] = nn.Linear(4096, num_classes)\nvgg = vgg.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(vgg.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:53.459812Z","iopub.execute_input":"2025-11-27T06:22:53.460414Z","iopub.status.idle":"2025-11-27T06:22:57.563737Z","shell.execute_reply.started":"2025-11-27T06:22:53.460393Z","shell.execute_reply":"2025-11-27T06:22:57.563169Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:02<00:00, 245MB/s] \n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 12) Training Loop + Accuracy + Save Best Model\n# ==============================================\nnum_epochs =15\nbest_val_loss = float(\"inf\")\n\ndef multilabel_accuracy(outputs, labels, threshold=0.5):\n    \"\"\" Ø­Ø³Ø§Ø¨ accuracy ÙÙŠ Ø§Ù„ multi-label \"\"\"\n    preds = (torch.sigmoid(outputs) > threshold).float()\n    correct = (preds == labels).float().mean()\n    return correct.item()\n\nfor epoch in range(num_epochs):\n    # ------------------\n    # Training\n    # ------------------\n    vgg.train()\n    train_loss = 0.0\n\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = vgg(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * imgs.size(0)\n\n    train_loss /= len(train_loader.dataset)\n\n    # ------------------\n    # Validation\n    # ------------------\n    vgg.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            outputs = vgg(imgs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item() * imgs.size(0)\n            val_acc += multilabel_accuracy(outputs, labels)\n\n    val_loss /= len(val_loader.dataset)\n    val_acc /= len(val_loader)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Train Loss: {train_loss:.4f}  \"\n          f\"Val Loss: {val_loss:.4f}  \"\n          f\"Val Acc: {val_acc:.4f}\")\n\n    # ------------------\n    # Save Best Model\n    # ------------------\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(vgg.state_dict(), \"best_vgg16_chestxray.pth\")\n        print(\"âœ” Model saved (best so far)\")\n\nprint(\"Training Complete âœ…\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:01.544317Z","iopub.execute_input":"2025-11-27T06:23:01.544821Z","iopub.status.idle":"2025-11-27T08:02:35.663798Z","shell.execute_reply.started":"2025-11-27T06:23:01.544796Z","shell.execute_reply":"2025-11-27T08:02:35.662870Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/15] Train Loss: 0.3384  Val Loss: 0.2825  Val Acc: 0.9111\nâœ” Model saved (best so far)\nEpoch [2/15] Train Loss: 0.3051  Val Loss: 0.2889  Val Acc: 0.9065\nEpoch [3/15] Train Loss: 0.2847  Val Loss: 0.2514  Val Acc: 0.9131\nâœ” Model saved (best so far)\nEpoch [4/15] Train Loss: 0.2683  Val Loss: 0.2636  Val Acc: 0.9080\nEpoch [5/15] Train Loss: 0.2519  Val Loss: 0.2695  Val Acc: 0.9034\nEpoch [6/15] Train Loss: 0.2378  Val Loss: 0.2580  Val Acc: 0.9042\nEpoch [7/15] Train Loss: 0.2254  Val Loss: 0.2732  Val Acc: 0.9028\nEpoch [8/15] Train Loss: 0.2127  Val Loss: 0.2708  Val Acc: 0.8991\nEpoch [9/15] Train Loss: 0.2030  Val Loss: 0.2648  Val Acc: 0.9006\nEpoch [10/15] Train Loss: 0.1929  Val Loss: 0.2511  Val Acc: 0.9042\nâœ” Model saved (best so far)\nEpoch [11/15] Train Loss: 0.1853  Val Loss: 0.2597  Val Acc: 0.9009\nEpoch [12/15] Train Loss: 0.1766  Val Loss: 0.2427  Val Acc: 0.9070\nâœ” Model saved (best so far)\nEpoch [13/15] Train Loss: 0.1699  Val Loss: 0.2442  Val Acc: 0.9080\nEpoch [14/15] Train Loss: 0.1637  Val Loss: 0.2465  Val Acc: 0.9065\nEpoch [15/15] Train Loss: 0.1580  Val Loss: 0.2474  Val Acc: 0.9062\nTraining Complete âœ…\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\n\nimport os\nimport torch\n\ncheckpoint_path = \"vgg16_chestxray_checkpoint.pth\"\nfinal_model_path = \"best_vgg16_chestxray.pth\"\ntotal_epochs = 15  # final number of epoches\n\n# checkpoint\nstart_epoch = 0\nbest_val_loss = float(\"inf\")\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    vgg.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    start_epoch = checkpoint[\"epoch\"] + 1\n    best_val_loss = checkpoint[\"best_val_loss\"]\n    print(f\"âœ” Resuming training from epoch {start_epoch}, best_val_loss: {best_val_loss:.4f}\")\n\nfor epoch in range(start_epoch, total_epochs):\n    # ------------------\n    # Training\n    # ------------------\n    vgg.train()\n    train_loss = 0.0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = vgg(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * imgs.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    # ------------------\n    # Validation\n    # ------------------\n    vgg.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = vgg(imgs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * imgs.size(0)\n            val_acc += multilabel_accuracy(outputs, labels)\n    val_loss /= len(val_loader.dataset)\n    val_acc /= len(val_loader)\n\n    print(f\"Epoch [{epoch+1}/{total_epochs}] \"\n          f\"Train Loss: {train_loss:.4f}  \"\n          f\"Val Loss: {val_loss:.4f}  \"\n          f\"Val Acc: {val_acc:.4f}\")\n\n    # ------------------\n    # Save checkpoint  val_loss\n    # ------------------\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state_dict\": vgg.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"best_val_loss\": best_val_loss\n        }, checkpoint_path)\n        print(\"âœ” Checkpoint saved (best so far)\")\n\n# ------------------\n# Save final best model for inference\n# ------------------\ntorch.save(vgg.state_dict(), final_model_path)\nprint(f\"âœ” Final model saved as {final_model_path}\")\nprint(\"Training Complete âœ…\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T08:32:23.996421Z","iopub.execute_input":"2025-11-27T08:32:23.997239Z","iopub.status.idle":"2025-11-27T10:10:32.870395Z","shell.execute_reply.started":"2025-11-27T08:32:23.997208Z","shell.execute_reply":"2025-11-27T10:10:32.869345Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/15] Train Loss: 0.1509  Val Loss: 0.2378  Val Acc: 0.9114\nâœ” Checkpoint saved (best so far)\nEpoch [2/15] Train Loss: 0.1467  Val Loss: 0.2551  Val Acc: 0.9036\nEpoch [3/15] Train Loss: 0.1431  Val Loss: 0.2403  Val Acc: 0.9086\nEpoch [4/15] Train Loss: 0.1382  Val Loss: 0.2395  Val Acc: 0.9100\nEpoch [5/15] Train Loss: 0.1341  Val Loss: 0.2450  Val Acc: 0.9064\nEpoch [6/15] Train Loss: 0.1303  Val Loss: 0.2291  Val Acc: 0.9146\nâœ” Checkpoint saved (best so far)\nEpoch [7/15] Train Loss: 0.1270  Val Loss: 0.2369  Val Acc: 0.9117\nEpoch [8/15] Train Loss: 0.1247  Val Loss: 0.2421  Val Acc: 0.9078\nEpoch [9/15] Train Loss: 0.1212  Val Loss: 0.2398  Val Acc: 0.9083\nEpoch [10/15] Train Loss: 0.1190  Val Loss: 0.2386  Val Acc: 0.9112\nEpoch [11/15] Train Loss: 0.1152  Val Loss: 0.2412  Val Acc: 0.9091\nEpoch [12/15] Train Loss: 0.1141  Val Loss: 0.2357  Val Acc: 0.9115\nEpoch [14/15] Train Loss: 0.1087  Val Loss: 0.2413  Val Acc: 0.9089\nEpoch [15/15] Train Loss: 0.1081  Val Loss: 0.2422  Val Acc: 0.9087\nâœ” Final model saved as best_vgg16_chestxray.pth\nTraining Complete âœ…\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ================================\n# 13) Test Set + Test Loader\n# ================================\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import recall_score\n\n# take 1/2 of val_df to be Test set\ntest_df, _ = train_test_split(val_df, test_size=0.5, random_state=42)\n\n# convert labels to multi-label binary format like train\nmlb = MultiLabelBinarizer(classes=classes)  # classes like train\ntest_labels = mlb.fit_transform(test_df['Finding Labels'])\n\n# Test Dataset + DataLoader\ntest_dataset = ChestXrayDataset(test_df, test_labels, BASE, selected_folders, transform=val_transform)\ntest_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n\nprint(f\"Test size: {len(test_dataset)} images\")\n\n# ================================\n# 14) Test Evaluation (Loss + Accuracy)\n# ================================\nvgg.eval()\ntest_loss = 0.0\ntest_acc  = 0.0\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for imgs, labels in test_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        outputs = vgg(imgs)\n        loss = criterion(outputs, labels)\n\n        test_loss += loss.item() * imgs.size(0)\n        test_acc  += multilabel_accuracy(outputs, labels)\n\n        # All expectations and labels for final calculations\n        preds = (torch.sigmoid(outputs) > 0.5).int().cpu().numpy()\n        all_preds.append(preds)\n        all_labels.append(labels.cpu().numpy())\n\ntest_loss /= len(test_loader.dataset)\ntest_acc  /= len(test_loader)\n\nall_preds = np.vstack(all_preds)\nall_labels = np.vstack(all_labels)\n\nprint(f\"\\nðŸ§ª Test Loss: {test_loss:.4f}\")\nprint(f\"ðŸ§ª Test Accuracy: {test_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:36:15.438147Z","iopub.execute_input":"2025-11-27T10:36:15.438893Z","iopub.status.idle":"2025-11-27T10:37:07.121791Z","shell.execute_reply.started":"2025-11-27T10:36:15.438859Z","shell.execute_reply":"2025-11-27T10:37:07.121149Z"}},"outputs":[{"name":"stdout","text":"Test size: 2250 images\n\nðŸ§ª Test Loss: 0.2441\nðŸ§ª Test Accuracy: 0.9092\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torchvision import transforms\n\n# ================================\n# 1) Transform  train/val\n# ================================\npredict_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# ================================\n# 2) Load Image\n# ================================\ndef predict_image(img_path, model, classes, device, threshold=0.5):\n    image = Image.open(img_path).convert(\"RGB\")\n    image = predict_transform(image).unsqueeze(0)  \n    image = image.to(device)\n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(image)\n        probs = torch.sigmoid(outputs).cpu().numpy()[0]\n\n    #  threshold\n    preds = {cls: float(prob) for cls, prob in zip(classes, probs) if prob > threshold}\n\n    #  thresholdØŒ  No Finding\n    if not preds:\n        preds = {\"No Finding\": 1.0}\n\n    return preds\n\n# ================================\n# 3) Example usage\n# ================================\nimg_path = \"/kaggle/input/data/images_006/images/00011558_010.png\"  \npreds = predict_image(img_path, vgg, classes, device, threshold=0.5)\n\nprint(\"Prediction results:\")\nfor disease, prob in preds.items():\n    print(f\"{disease}: {prob:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:53.426045Z","iopub.execute_input":"2025-11-27T10:42:53.426669Z","iopub.status.idle":"2025-11-27T10:42:53.500516Z","shell.execute_reply.started":"2025-11-27T10:42:53.426646Z","shell.execute_reply":"2025-11-27T10:42:53.499934Z"}},"outputs":[{"name":"stdout","text":"Prediction results:\nAtelectasis: 0.6308\nInfiltration: 0.6191\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load best saved model\nvgg.load_state_dict(torch.load(\"best_vgg16_chestxray.pth\", map_location=device))\nvgg.to(device)\nvgg.eval()\n\nprint(\"âœ” Best model loaded and ready for inference!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:27:08.429687Z","iopub.execute_input":"2025-11-27T11:27:08.430464Z","iopub.status.idle":"2025-11-27T11:27:08.886397Z","shell.execute_reply.started":"2025-11-27T11:27:08.430437Z","shell.execute_reply":"2025-11-27T11:27:08.885603Z"}},"outputs":[{"name":"stdout","text":"âœ” Best model loaded and ready for inference!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# link for downloading the model\nfrom IPython.display import FileLink\nFileLink(\"best_vgg16_chestxray.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:30:41.288947Z","iopub.execute_input":"2025-11-27T11:30:41.289749Z","iopub.status.idle":"2025-11-27T11:30:41.295147Z","shell.execute_reply.started":"2025-11-27T11:30:41.289723Z","shell.execute_reply":"2025-11-27T11:30:41.294428Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/best_vgg16_chestxray.pth","text/html":"<a href='best_vgg16_chestxray.pth' target='_blank'>best_vgg16_chestxray.pth</a><br>"},"metadata":{}}],"execution_count":25}]}